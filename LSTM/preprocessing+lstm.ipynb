{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 6.5 MB 7.4 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in ./anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
      "  Downloading pydantic-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 12.7 MB 40.4 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./anaconda3/lib/python3.8/site-packages (from spacy) (4.50.2)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001B[K     |████████████████████████████████| 130 kB 35.4 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.8/site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.8/site-packages (from spacy) (50.3.1.post20201107)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./anaconda3/lib/python3.8/site-packages (from spacy) (1.19.2)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (835 kB)\n",
      "\u001B[K     |████████████████████████████████| 835 kB 43.0 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "\u001B[K     |████████████████████████████████| 42 kB 1.7 MB/s  eta 0:00:01\n",
      "\u001B[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001B[K     |████████████████████████████████| 181 kB 39.7 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in ./anaconda3/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001B[K     |████████████████████████████████| 459 kB 73.4 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: chardet<4,>=3.0.2 in ./anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001B[K     |████████████████████████████████| 10.2 MB 26.5 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001B[K     |████████████████████████████████| 58 kB 7.7 MB/s  eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: MarkupSafe>=0.23 in ./anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: pydantic, wasabi, cymem, murmurhash, preshed, spacy-legacy, spacy-loggers, typer, catalogue, srsly, blis, thinc, smart-open, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.8 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.2 preshed-3.0.6 pydantic-1.9.1 smart-open-5.2.1 spacy-3.4.0 spacy-legacy-3.0.9 spacy-loggers-1.0.3 srsly-2.4.3 thinc-8.1.0 typer-0.4.2 wasabi-0.9.1\n"
     ]
    }
   ],
   "source": [
    "#Envorinment setting\n",
    "#!pip install spacy\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "#!pip install torchtext==0.9.0\n",
    "#!pip install torch==1.7.1\n",
    "#!conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=10.1 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /userhome/cs/u3591571/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install demoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# !pip install demoji\n",
    "nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "import re\n",
    "import unicodedata\n",
    "import csv\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "SEED = 1024\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import torchtext\n",
    "from torchtext import legacy\n",
    "from torchtext import *\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, label_ranking_average_precision_score, f1_score,accuracy_score\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf \"COVID19_sentinentanalysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'COVID19_sentinentanalysis' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/xingxingaaa/COVID19_sentinentanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Senwave Dataset = 10000\n"
     ]
    }
   ],
   "source": [
    "senwave = pd.read_csv(\"labeledEn.csv\")\n",
    "print(\"Length of Senwave Dataset = {}\".format(len(senwave)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
    "\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "\"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "contractionsWithAnotherInvertedComma = { \n",
    "\"ain’t\": \"am not\", \"aren’t\": \"are not\", \"can’t\": \"cannot\", \"can’t’ve\": \"cannot have\", \"’cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\",\n",
    "\"couldn’t’ve\": \"could not have\", \"didn’t\": \"did not\", \"doesn’t\": \"does not\", \"don’t\": \"do not\", \"hadn’t\": \"had not\", \"hadn’t’ve\": \"had not have\",\n",
    "\"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he had\", \"he’d’ve\": \"he would have\", \"he’ll\": \"he will\", \"he’ll’ve\": \"he will have\", \"he’s\": \"he is\",\n",
    "\"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"how’s\": \"how is\", \"i’d\": \"i would\", \"i’d’ve\": \"i would have\",\n",
    "\"i’ll\": \"i will\", \"i’ll’ve\": \"i will have\", \"i’m\": \"i am\", \"i’ve\": \"i have\", \"isn’t\": \"is not\", \"it’d\": \"it would\",\n",
    "\"it’d’ve\": \"it would have\", \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\", \"it’s\": \"it is\", \"let’s\": \"let us\",\n",
    "\"ma’am\": \"madam\", \"mayn’t\": \"may not\", \"might’ve\": \"might have\", \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\",\n",
    "\"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\",\n",
    "\"shan’t\": \"shall not\", \"shan’t’ve\": \"shall not have\", \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"she’ll\": \"she will\",\n",
    "\"she’ll’ve\": \"she will have\", \"she’s\": \"she is\", \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\",\n",
    "\"so’ve\": \"so have\", \"so’s\": \"so is\", \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"that’s\": \"that is\", \"there’d\": \"there would\",\n",
    "\"there’d’ve\": \"there would have\", \"there’s\": \"there is\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\", \"they’ll\": \"they will\",\n",
    "\"they’ll’ve\": \"they will have\", \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\",\n",
    "\"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\", \"what’ll\": \"what will\",\n",
    "\"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "\"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’s\": \"where is\", \"where’ve\": \"where have\", \"who’ll\": \"who will\", \"who’ll’ve\": \"who will have\",\n",
    "\"who’s\": \"who is\", \"who’ve\": \"who have\", \"why’s\": \"why is\", \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \"won’t’ve\": \"will not have\",\n",
    "\"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\",\n",
    "\"y’all’re\": \"you all are\", \"y’all’ve\": \"you all have\", \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"you’ll’ve\": \"you will have\",\n",
    "\"you’re\": \"you are\", \"you’ve\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.preprocess import preprocess\n",
    "pp_class = preprocess(senwave, contractions, contractionsWithAnotherInvertedComma) ## this is a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "senwave['Tweet'] = senwave['Tweet'].apply(lambda x : pp_class.preprocess_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Optimistic</th>\n",
       "      <th>Thankful</th>\n",
       "      <th>Empathetic</th>\n",
       "      <th>Pessimistic</th>\n",
       "      <th>Anxious</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Annoyed</th>\n",
       "      <th>Denial</th>\n",
       "      <th>Official report</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Joking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1245138832040890370</td>\n",
       "      <td>a glass of wine keeps the corona away  drake  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245138859874234368</td>\n",
       "      <td>can anyone tell me if you took the flu shot la...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245138869353353222</td>\n",
       "      <td>by the way producers send me beats im working ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245138873740648448</td>\n",
       "      <td>when someone you know   apart of your family d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1245138886172585989</td>\n",
       "      <td>dear soccer  i really miss you  please come ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1245138911195803649</td>\n",
       "      <td>new home remedy to treat coronavirus  tested b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1245138920934969344</td>\n",
       "      <td>when xavier wulf does an attack on titan tape ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1245138945119223814</td>\n",
       "      <td>mouthwash is hand san for your mouth and i do ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1245138953302409216</td>\n",
       "      <td>yes all of them   n france 1 000 christians to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1245138958222311431</td>\n",
       "      <td>update i destroyed the tire honestly if i get ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                              Tweet  \\\n",
       "0  1245138832040890370  a glass of wine keeps the corona away  drake  ...   \n",
       "1  1245138859874234368  can anyone tell me if you took the flu shot la...   \n",
       "2  1245138869353353222  by the way producers send me beats im working ...   \n",
       "3  1245138873740648448  when someone you know   apart of your family d...   \n",
       "4  1245138886172585989  dear soccer  i really miss you  please come ba...   \n",
       "5  1245138911195803649  new home remedy to treat coronavirus  tested b...   \n",
       "6  1245138920934969344  when xavier wulf does an attack on titan tape ...   \n",
       "7  1245138945119223814  mouthwash is hand san for your mouth and i do ...   \n",
       "8  1245138953302409216  yes all of them   n france 1 000 christians to...   \n",
       "9  1245138958222311431  update i destroyed the tire honestly if i get ...   \n",
       "\n",
       "   Optimistic  Thankful  Empathetic  Pessimistic  Anxious  Sad  Annoyed  \\\n",
       "0           1         0           0            0        0    0        0   \n",
       "1           0         0           0            0        1    0        0   \n",
       "2           1         0           0            0        0    0        0   \n",
       "3           0         0           0            0        0    1        0   \n",
       "4           0         0           0            0        0    1        1   \n",
       "5           1         0           0            0        0    0        0   \n",
       "6           0         0           0            0        0    0        0   \n",
       "7           0         0           0            0        0    1        0   \n",
       "8           0         0           0            0        0    0        1   \n",
       "9           0         0           0            1        0    1        0   \n",
       "\n",
       "   Denial  Official report  Surprise  Joking  \n",
       "0       0                0         0       1  \n",
       "1       0                0         1       0  \n",
       "2       0                0         0       1  \n",
       "3       0                0         0       0  \n",
       "4       0                0         0       0  \n",
       "5       1                0         0       1  \n",
       "6       0                0         0       1  \n",
       "7       0                0         1       1  \n",
       "8       0                1         1       0  \n",
       "9       0                0         0       0  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senwave['Tweet'] = senwave['Tweet'].str.lower()\n",
    "senwave.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import wordReplace\n",
    "senwave['Tweet'] = senwave['Tweet'].apply(lambda x : wordReplace.bruteGen(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "senwave.to_csv('senwave_preprocess.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for 11 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "senwave = pd.read_csv('senwave_preprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "senwave = senwave.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sen_train, sen_test = train_test_split(senwave, train_size = 0.9, random_state = 42)\n",
    "\n",
    "sen_train.to_csv(\"train.csv\", index = False)\n",
    "sen_test.to_csv(\"test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Spacy 会先将文档分解成句子，然后再tokenize。我们可以使用迭代来遍历整个文档\n",
    "def tokenizer(tweet):\n",
    "    tweet = re.sub(r'[\\n]', ' ', tweet)\n",
    "    return [tok.text for tok in spacy_en.tokenizer(tweet)] \n",
    "\n",
    "TWEET = torchtext.legacy.data.Field(sequential = True, lower = True, tokenize = tokenizer)  #tokenize 类型: function，作用: 文本处理，默认为str.split()\n",
    "LABEL = torchtext.legacy.data.Field(sequential = False, use_vocab = False)\n",
    "\n",
    "# 设置表头\n",
    "dataFields = [(\"ID\", None), (\"Tweet\", TWEET), (\"Optimistic\", LABEL), (\"Thankful\", LABEL),\n",
    "              (\"Empathetic\", LABEL), (\"Pessimistic\", LABEL), (\"Anxious\", LABEL), (\"Sad\", LABEL),\n",
    "              (\"Annoyed\", LABEL), (\"Denial\", LABEL), (\"Official report\", LABEL),\n",
    "              (\"Surprise\", LABEL), (\"Joking\", LABEL)]\n",
    "# 读取数据\n",
    "train_dataset, test_dataset = torchtext.legacy.data.TabularDataset.splits(\n",
    "    path = '', train = 'train.csv', test = 'test.csv', format = 'csv', fields = dataFields, skip_header = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词汇表\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TWEET.build_vocab(train_dataset, vectors = 'glove.840B.300d') # get word embedding\n",
    "# vectors, One of either the available pretrained vectors or custom pretrained vectors\n",
    "# 然后构建语料库的 Vocabulary， 同时，加载预训练的 word-embedding(TEXT 字段的文本转化为数值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改TWEET.vocab\n",
    "np.random.seed(1024)\n",
    "for i in range(TWEET.vocab.vectors.shape[0]):\n",
    "    vec = TWEET.vocab.vectors[i]\n",
    "    if torch.sum(vec).item() == 0:\n",
    "        a = np.random.uniform(-0.25, 0.25, 300)\n",
    "        TWEET.vocab.vectors[i] = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "vocab = TWEET.vocab # 获得词汇表\n",
    "BATCH_SIZE = 32\n",
    "n_label= 11\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# 文本批处理，即一批一批地读取数据\n",
    "train_iter, test_iter = legacy.data.BucketIterator.splits(datasets = (train_dataset, test_dataset),\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   sort_key = lambda x : len(x.Tweet),\n",
    "                                                   sort_within_batch = False,\n",
    "                                                   repeat = False,\n",
    "                                                   device = device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.legacy.data.batch.Batch of size 32]\n",
       "\t[.Tweet]:[torch.cuda.LongTensor of size 28x32 (GPU 0)]\n",
       "\t[.Optimistic]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Thankful]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Empathetic]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Pessimistic]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Anxious]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Sad]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Annoyed]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Denial]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Official report]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Surprise]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Joking]:[torch.cuda.LongTensor of size 32 (GPU 0)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea taken from http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "# 对batch做个包装，方便调用\n",
    "class BatchWrapper():\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl = dl\n",
    "        self.x_var = x_var\n",
    "        self.y_vars = y_vars\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var)\n",
    "            if self.y_vars is not None:\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim = 1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "            yield(x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"Tweet\", ['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', \n",
    "                                              'Anxious', 'Sad', 'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'])\n",
    "test_dl = BatchWrapper(test_iter, \"Tweet\", ['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', \n",
    "                                            'Anxious', 'Sad', 'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab, hidden_dim, output_dim, drop_prob, bidirectional = False, use_glove = True):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(len(vocab), embedding_dim) # vocab_size词汇表大小， embedding_dim词嵌入维度\n",
    "        \n",
    "        if use_glove:\n",
    "            self.embeddings.weight.data.copy_(vocab.vectors) # 如果使用glove\n",
    "            self.embeddings.weight.requires_grad = False\n",
    "            \n",
    "        self.drop_prob = drop_prob # drop layer\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional = bidirectional, batch_first = True, num_layers = 2)\n",
    "        \n",
    "        if bidirectional is True:\n",
    "            self.lin = nn.Linear(2*hidden_dim, 64)\n",
    "        else:\n",
    "            self.lin = nn.Linear(hidden_dim, 64) # 加入线性层\n",
    "        \n",
    "        self.fc = nn.Linear(64, output_dim) #这里是没有过sigmoid的\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = drop_prob)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        #sentence = [max_len, batch_size]\n",
    "\n",
    "        embed = self.embeddings(torch.transpose(sentence, 0, 1))\n",
    "        #embed = [batch_size, max_len, embedding_dim]\n",
    "        \n",
    "        if self.drop_prob:\n",
    "            embed = self.dropout(embed)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(embed)\n",
    "        #lstm_out = [batch_size, max_len, 2*hidden_dim if bidirectional else hidden_dim]\n",
    "        #hidden = [num_layers, batch_size, hidden_dim]\n",
    "        #cell = [num_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        out = lstm_out[:,-1,:].squeeze()\n",
    "        #out = [batch_size, 2*hidden_dim if bidirectional else hidden_dim]\n",
    "        \n",
    "        out = self.lin(out)\n",
    "        #out = [batch_size, 64]\n",
    "\n",
    "        outputs = self.fc(out)\n",
    "        #outputs = [batch_size, output_dim]\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "def evaluation_metrics(actual_labels, pred_labels, threshold):\n",
    "    int_pred_labels = pred_labels\n",
    "    for i in range(len(pred_labels)):\n",
    "        for j in range(11):\n",
    "            if int_pred_labels[i][j] >= threshold: int_pred_labels[i][j] = 1\n",
    "            else:\n",
    "                int_pred_labels[i][j] = 0\n",
    "    \n",
    "    ham_loss = hamming_loss(actual_labels, int_pred_labels)\n",
    "    accuracy_scores = accuracy_score(actual_labels, int_pred_labels)\n",
    "    jacc_score = jaccard_score(actual_labels, int_pred_labels, average = 'samples')\n",
    "    lrap = label_ranking_average_precision_score(actual_labels, pred_labels)\n",
    "    f1_macro = f1_score(actual_labels, int_pred_labels, average = 'macro')\n",
    "    f1_micro = f1_score(actual_labels, int_pred_labels, average = 'micro')\n",
    "\n",
    "    return ham_loss,accuracy_scores, jacc_score, lrap, f1_macro, f1_micro\n",
    "\n",
    "def train(model, loss_fn, optimizer, n_epochs, train_dl, threshold,test_dl):\n",
    "\n",
    "    train_losses = []  \n",
    "    accuracy = []\n",
    "    hamming_losses = []\n",
    "    jaccard_scores = []\n",
    "    lraps = []  \n",
    "    iter = 1\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        running_loss = 0.0\n",
    "        pred_labels = []\n",
    "        actual_labels = []\n",
    "        model.train()\n",
    "        for x, y in train_dl:\n",
    "            # print(x.shape, y.shape) # torch.Size([27, 32]) torch.Size([32, 3])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(x) # 每一个类别输出了一个值\n",
    "            # print('preds',preds)\n",
    "            m = nn.Sigmoid()\n",
    "            \n",
    "            sig_preds = m(preds) # model输出值过了一个sigmoid, 在lstm当中是不会过sigmoid的吗\n",
    "            # print('sig_preds',sig_preds)\n",
    "            \n",
    "            for tens in sig_preds:\n",
    "                pred_labels.append(tens.cpu().detach().numpy())\n",
    "            for tens in y:\n",
    "                actual_labels.append(tens.cpu().detach().numpy())\n",
    "\n",
    "            loss = loss_fn(preds, y) # preds是模型的输出值,y就是真实label\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x.shape[0]\n",
    "\n",
    "        ham_loss, accuracy_scores,jacc_score, lrap, f1_macro, f1_micro = evaluation_metrics(actual_labels, pred_labels, threshold)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        accuracy.append(accuracy_scores)\n",
    "        hamming_losses.append(ham_loss)\n",
    "        lraps.append(lrap)\n",
    "        jaccard_scores.append(jacc_score)\n",
    "        \n",
    "        if iter % 2 == 0:\n",
    "            print(\"Epoch: \", epoch)\n",
    "            print(\"Binary Cross Entropy With Logits Loss: {:.4f}   \".format(epoch_loss),\n",
    "                  \"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "            test_loss, accuracy_scores,test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
    "            print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}   \".format(test_loss),\n",
    "                 \"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "            print(\"\\n\")\n",
    "        iter += 1\n",
    "\n",
    "    return train_losses,accuracy, hamming_losses, jaccard_scores, lraps, f1_macro, f1_micro\n",
    "\n",
    "def test(model, loss_fn, test_dl, threshold):\n",
    "    running_loss = 0.0\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    model.eval()\n",
    "    for x, y in test_dl:\n",
    "        #print(x.shape, y.shape)\n",
    "\n",
    "        preds = model(x)\n",
    "\n",
    "        m = nn.Sigmoid()\n",
    "        sig_preds = m(preds)\n",
    "        \n",
    "        for tens in sig_preds:\n",
    "            pred_labels.append(tens.cpu().detach().numpy())\n",
    "        for tens in y:\n",
    "            actual_labels.append(tens.cpu().detach().numpy())\n",
    "\n",
    "        loss = loss_fn(preds, y)\n",
    "\n",
    "        running_loss += loss.item() * x.shape[0]\n",
    "\n",
    "    ham_loss, accuracy_scores, jacc_score, lrap, f1_macro, f1_micro = evaluation_metrics(actual_labels, pred_labels, threshold)\n",
    "\n",
    "    test_loss = running_loss / len(test_dataset)\n",
    "    return test_loss,accuracy_scores, ham_loss, jacc_score, lrap, f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm with glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Binary Cross Entropy With Logits Loss: 0.3688    accuracy Score: 0.0787\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2491    accuracy Score: 0.0420\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Binary Cross Entropy With Logits Loss: 0.3556    accuracy Score: 0.1111\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2455    accuracy Score: 0.1060\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "Binary Cross Entropy With Logits Loss: 0.3525    accuracy Score: 0.1150\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2429    accuracy Score: 0.1180\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "Binary Cross Entropy With Logits Loss: 0.3463    accuracy Score: 0.1183\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2375    accuracy Score: 0.1270\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "Binary Cross Entropy With Logits Loss: 0.3321    accuracy Score: 0.1413\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2315    accuracy Score: 0.1490\n",
      "\n",
      "\n",
      "Epoch:  12\n",
      "Binary Cross Entropy With Logits Loss: 0.3234    accuracy Score: 0.1552\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2263    accuracy Score: 0.1740\n",
      "\n",
      "\n",
      "Epoch:  14\n",
      "Binary Cross Entropy With Logits Loss: 0.3178    accuracy Score: 0.1660\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2254    accuracy Score: 0.1810\n",
      "\n",
      "\n",
      "Epoch:  16\n",
      "Binary Cross Entropy With Logits Loss: 0.3106    accuracy Score: 0.1812\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2197    accuracy Score: 0.1860\n",
      "\n",
      "\n",
      "Epoch:  18\n",
      "Binary Cross Entropy With Logits Loss: 0.3069    accuracy Score: 0.1900\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2228    accuracy Score: 0.1800\n",
      "\n",
      "\n",
      "Epoch:  20\n",
      "Binary Cross Entropy With Logits Loss: 0.3021    accuracy Score: 0.1969\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2192    accuracy Score: 0.2030\n",
      "\n",
      "\n",
      "Epoch:  22\n",
      "Binary Cross Entropy With Logits Loss: 0.2974    accuracy Score: 0.2009\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2164    accuracy Score: 0.2040\n",
      "\n",
      "\n",
      "Epoch:  24\n",
      "Binary Cross Entropy With Logits Loss: 0.2919    accuracy Score: 0.2097\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2192    accuracy Score: 0.2160\n",
      "\n",
      "\n",
      "Epoch:  26\n",
      "Binary Cross Entropy With Logits Loss: 0.2892    accuracy Score: 0.2164\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2187    accuracy Score: 0.2190\n",
      "\n",
      "\n",
      "Epoch:  28\n",
      "Binary Cross Entropy With Logits Loss: 0.2845    accuracy Score: 0.2213\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2204    accuracy Score: 0.2070\n",
      "\n",
      "\n",
      "Epoch:  30\n",
      "Binary Cross Entropy With Logits Loss: 0.2797    accuracy Score: 0.2376\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2218    accuracy Score: 0.2200\n",
      "\n",
      "\n",
      "Epoch:  32\n",
      "Binary Cross Entropy With Logits Loss: 0.2756    accuracy Score: 0.2339\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2265    accuracy Score: 0.2100\n",
      "\n",
      "\n",
      "Epoch:  34\n",
      "Binary Cross Entropy With Logits Loss: 0.2717    accuracy Score: 0.2412\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2248    accuracy Score: 0.2040\n",
      "\n",
      "\n",
      "Epoch:  36\n",
      "Binary Cross Entropy With Logits Loss: 0.2707    accuracy Score: 0.2444\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2277    accuracy Score: 0.2040\n",
      "\n",
      "\n",
      "Epoch:  38\n",
      "Binary Cross Entropy With Logits Loss: 0.2652    accuracy Score: 0.2520\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2317    accuracy Score: 0.2170\n",
      "\n",
      "\n",
      "Epoch:  40\n",
      "Binary Cross Entropy With Logits Loss: 0.2625    accuracy Score: 0.2610\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2303    accuracy Score: 0.2140\n",
      "\n",
      "\n",
      "------------------------Final Train result-------------------\n",
      "Train result\n",
      "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout = 0.65\n",
      "Binary Cross Entropy With Logits Loss: 0.2625\n",
      "Hamming Loss : 0.1243\n",
      "Jaccard Score: 0.5117\n",
      "accuracy Score: 0.2610\n",
      "Label Ranking Average Precision Score: 0.5951\n",
      "F1 Macro Score: 0.5056\n",
      "F1 Micro Score: 0.5900\n",
      "\n",
      "\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2303\n",
      "Test Hamming Loss : 0.1469\n",
      "Test Jaccard Score: 0.4373\n",
      "accuracy Score: 0.2140\n",
      "Test Label Ranking Average Precision Score: 0.5262\n",
      "Test F1 Macro Score: 0.4007\n",
      "Test F1 Micro Score: 0.5109\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# lstm with glove\n",
    "learning_rates = [1e-3]\n",
    "hidden_dims = [128]\n",
    "thresholds= [0.5]\n",
    "dropouts = [0.65]\n",
    "n_epoch = 40\n",
    "device = torch.device('cuda')\n",
    "all_models = []\n",
    "iter = 1\n",
    "\n",
    "#number of iterations = 3*2*2 = 12 = number of models\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for threshold in thresholds:\n",
    "            for dropout in dropouts:\n",
    "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = dropout)\n",
    "                model = model.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "                train_loss, accuracy,hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                all_models.append(model)\n",
    "                print(\"------------------------Final Train result-------------------\")\n",
    "                print(\"Train result\")\n",
    "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout = {}\".format(learning_rate, hidden_dim, threshold, dropout))\n",
    "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
    "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
    "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
    "                print(\"accuracy Score: {:.4f}\".format(max(accuracy)))\n",
    "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
    "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
    "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
    "                print(\"\\n\")\n",
    "\n",
    "                test_loss, accuracy_scores,test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
    "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
    "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
    "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
    "                print(\"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
    "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
    "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
    "                print(\"\\n\")\n",
    "                print(\"----------------------------------------------------------------\")\n",
    "                iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm without glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Binary Cross Entropy With Logits Loss: 0.3863    accuracy Score: 0.0136\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2637    accuracy Score: 0.0730\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Binary Cross Entropy With Logits Loss: 0.3652    accuracy Score: 0.1003\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2590    accuracy Score: 0.1220\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "Binary Cross Entropy With Logits Loss: 0.3562    accuracy Score: 0.1064\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2604    accuracy Score: 0.1170\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "Binary Cross Entropy With Logits Loss: 0.3509    accuracy Score: 0.1119\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2575    accuracy Score: 0.1210\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "Binary Cross Entropy With Logits Loss: 0.3444    accuracy Score: 0.1146\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2585    accuracy Score: 0.1210\n",
      "\n",
      "\n",
      "Epoch:  12\n",
      "Binary Cross Entropy With Logits Loss: 0.3413    accuracy Score: 0.1228\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2574    accuracy Score: 0.1280\n",
      "\n",
      "\n",
      "Epoch:  14\n",
      "Binary Cross Entropy With Logits Loss: 0.3347    accuracy Score: 0.1353\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2556    accuracy Score: 0.1360\n",
      "\n",
      "\n",
      "Epoch:  16\n",
      "Binary Cross Entropy With Logits Loss: 0.3297    accuracy Score: 0.1510\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2579    accuracy Score: 0.1490\n",
      "\n",
      "\n",
      "Epoch:  18\n",
      "Binary Cross Entropy With Logits Loss: 0.3228    accuracy Score: 0.1571\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2571    accuracy Score: 0.1460\n",
      "\n",
      "\n",
      "Epoch:  20\n",
      "Binary Cross Entropy With Logits Loss: 0.3165    accuracy Score: 0.1709\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2568    accuracy Score: 0.1590\n",
      "\n",
      "\n",
      "Epoch:  22\n",
      "Binary Cross Entropy With Logits Loss: 0.3103    accuracy Score: 0.1808\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2541    accuracy Score: 0.1660\n",
      "\n",
      "\n",
      "Epoch:  24\n",
      "Binary Cross Entropy With Logits Loss: 0.3048    accuracy Score: 0.1850\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2550    accuracy Score: 0.1810\n",
      "\n",
      "\n",
      "Epoch:  26\n",
      "Binary Cross Entropy With Logits Loss: 0.2990    accuracy Score: 0.1981\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2597    accuracy Score: 0.1770\n",
      "\n",
      "\n",
      "Epoch:  28\n",
      "Binary Cross Entropy With Logits Loss: 0.2932    accuracy Score: 0.2047\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2599    accuracy Score: 0.1830\n",
      "\n",
      "\n",
      "Epoch:  30\n",
      "Binary Cross Entropy With Logits Loss: 0.2897    accuracy Score: 0.2091\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2633    accuracy Score: 0.1810\n",
      "\n",
      "\n",
      "Epoch:  32\n",
      "Binary Cross Entropy With Logits Loss: 0.2838    accuracy Score: 0.2183\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2670    accuracy Score: 0.1870\n",
      "\n",
      "\n",
      "Epoch:  34\n",
      "Binary Cross Entropy With Logits Loss: 0.2797    accuracy Score: 0.2264\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2630    accuracy Score: 0.1770\n",
      "\n",
      "\n",
      "Epoch:  36\n",
      "Binary Cross Entropy With Logits Loss: 0.2760    accuracy Score: 0.2314\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2626    accuracy Score: 0.1840\n",
      "\n",
      "\n",
      "Epoch:  38\n",
      "Binary Cross Entropy With Logits Loss: 0.2722    accuracy Score: 0.2354\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2740    accuracy Score: 0.1800\n",
      "\n",
      "\n",
      "Epoch:  40\n",
      "Binary Cross Entropy With Logits Loss: 0.2664    accuracy Score: 0.2514\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2707    accuracy Score: 0.1830\n",
      "\n",
      "\n",
      "------------------------Final Train result-------------------\n",
      "Train result\n",
      "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout = 0.65\n",
      "Binary Cross Entropy With Logits Loss: 0.2664\n",
      "Hamming Loss : 0.1245\n",
      "Jaccard Score: 0.4984\n",
      "accuracy Score: 0.2514\n",
      "Label Ranking Average Precision Score: 0.5858\n",
      "F1 Macro Score: 0.4307\n",
      "F1 Micro Score: 0.5817\n",
      "\n",
      "\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2707\n",
      "Test Hamming Loss : 0.1615\n",
      "Test Jaccard Score: 0.3917\n",
      "accuracy Score: 0.1830\n",
      "Test Label Ranking Average Precision Score: 0.4901\n",
      "Test F1 Macro Score: 0.2968\n",
      "Test F1 Micro Score: 0.4594\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# lstm without glove\n",
    "learning_rates = [1e-3]\n",
    "hidden_dims = [128]\n",
    "thresholds = [0.5]\n",
    "dropouts = [0.65]\n",
    "n_epoch = 40\n",
    "device = torch.device('cuda')\n",
    "all_models = []\n",
    "iter = 1\n",
    "\n",
    "#number of iterations = 3*2*2 = 12 = number of models\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for threshold in thresholds:\n",
    "            for dropout in dropouts:\n",
    "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = dropout,use_glove = False)\n",
    "                model = model.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "                train_loss, accuracy,hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                all_models.append(model)\n",
    "                print(\"------------------------Final Train result-------------------\")\n",
    "                print(\"Train result\")\n",
    "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout = {}\".format(learning_rate, hidden_dim, threshold, dropout))\n",
    "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
    "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
    "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
    "                print(\"accuracy Score: {:.4f}\".format(max(accuracy)))\n",
    "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
    "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
    "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
    "                print(\"\\n\")\n",
    "\n",
    "                test_loss, accuracy_scores,test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
    "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
    "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
    "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
    "                print(\"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
    "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
    "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
    "                print(\"\\n\")\n",
    "                print(\"----------------------------------------------------------------\")\n",
    "                iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bi- lstm with glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Binary Cross Entropy With Logits Loss: 0.3704    accuracy Score: 0.0846\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2469    accuracy Score: 0.1100\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Binary Cross Entropy With Logits Loss: 0.3561    accuracy Score: 0.1079\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2448    accuracy Score: 0.1200\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "Binary Cross Entropy With Logits Loss: 0.3511    accuracy Score: 0.1147\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2422    accuracy Score: 0.1010\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "Binary Cross Entropy With Logits Loss: 0.3363    accuracy Score: 0.1402\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2324    accuracy Score: 0.1580\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "Binary Cross Entropy With Logits Loss: 0.3272    accuracy Score: 0.1566\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2322    accuracy Score: 0.1510\n",
      "\n",
      "\n",
      "Epoch:  12\n",
      "Binary Cross Entropy With Logits Loss: 0.3205    accuracy Score: 0.1650\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2289    accuracy Score: 0.1660\n",
      "\n",
      "\n",
      "Epoch:  14\n",
      "Binary Cross Entropy With Logits Loss: 0.3164    accuracy Score: 0.1762\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2269    accuracy Score: 0.1620\n",
      "\n",
      "\n",
      "Epoch:  16\n",
      "Binary Cross Entropy With Logits Loss: 0.3100    accuracy Score: 0.1809\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2252    accuracy Score: 0.1760\n",
      "\n",
      "\n",
      "Epoch:  18\n",
      "Binary Cross Entropy With Logits Loss: 0.3040    accuracy Score: 0.1913\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2254    accuracy Score: 0.1940\n",
      "\n",
      "\n",
      "Epoch:  20\n",
      "Binary Cross Entropy With Logits Loss: 0.3010    accuracy Score: 0.1983\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2299    accuracy Score: 0.1830\n",
      "\n",
      "\n",
      "Epoch:  22\n",
      "Binary Cross Entropy With Logits Loss: 0.2963    accuracy Score: 0.2038\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2265    accuracy Score: 0.1920\n",
      "\n",
      "\n",
      "Epoch:  24\n",
      "Binary Cross Entropy With Logits Loss: 0.2919    accuracy Score: 0.2087\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2327    accuracy Score: 0.1850\n",
      "\n",
      "\n",
      "------------------------Final Train result-------------------\n",
      "Train result\n",
      "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout = 0.65\n",
      "Binary Cross Entropy With Logits Loss: 0.2898\n",
      "Hamming Loss : 0.1382\n",
      "Jaccard Score: 0.4397\n",
      "accuracy Score: 0.2154\n",
      "Label Ranking Average Precision Score: 0.5413\n",
      "F1 Macro Score: 0.4049\n",
      "F1 Micro Score: 0.5176\n",
      "\n",
      "\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2316\n",
      "Test Hamming Loss : 0.1538\n",
      "Test Jaccard Score: 0.3951\n",
      "accuracy Score: 0.1870\n",
      "Test Label Ranking Average Precision Score: 0.4964\n",
      "Test F1 Macro Score: 0.3515\n",
      "Test F1 Micro Score: 0.4669\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# bi- lstm with glove\n",
    "learning_rates = [1e-3]\n",
    "hidden_dims = [128]\n",
    "thresholds = [0.5]\n",
    "dropouts = [0.65]\n",
    "n_epoch = 25\n",
    "device = torch.device('cuda')\n",
    "all_models = []\n",
    "iter = 1\n",
    "\n",
    "#number of iterations = 3*2*2 = 12 = number of models\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for threshold in thresholds:\n",
    "            for dropout in dropouts:\n",
    "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = dropout,\n",
    "                                   bidirectional = True, use_glove = True)\n",
    "                model = model.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "                train_loss, accuracy,hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                all_models.append(model)\n",
    "                print(\"------------------------Final Train result-------------------\")\n",
    "                print(\"Train result\")\n",
    "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout = {}\".format(learning_rate, hidden_dim, threshold, dropout))\n",
    "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
    "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
    "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
    "                print(\"accuracy Score: {:.4f}\".format(max(accuracy)))\n",
    "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
    "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
    "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
    "                print(\"\\n\")\n",
    "\n",
    "                test_loss, accuracy_scores,test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
    "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
    "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
    "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
    "                print(\"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
    "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
    "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
    "                print(\"\\n\")\n",
    "                print(\"----------------------------------------------------------------\")\n",
    "                iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bi-lstm without glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Binary Cross Entropy With Logits Loss: 0.3891    accuracy Score: 0.0060\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2697    accuracy Score: 0.0500\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Binary Cross Entropy With Logits Loss: 0.3637    accuracy Score: 0.0976\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2532    accuracy Score: 0.1130\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "Binary Cross Entropy With Logits Loss: 0.3540    accuracy Score: 0.1096\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2502    accuracy Score: 0.1090\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "Binary Cross Entropy With Logits Loss: 0.3493    accuracy Score: 0.1127\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2486    accuracy Score: 0.1240\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "Binary Cross Entropy With Logits Loss: 0.3444    accuracy Score: 0.1177\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2492    accuracy Score: 0.1260\n",
      "\n",
      "\n",
      "Epoch:  12\n",
      "Binary Cross Entropy With Logits Loss: 0.3377    accuracy Score: 0.1232\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2470    accuracy Score: 0.1330\n",
      "\n",
      "\n",
      "Epoch:  14\n",
      "Binary Cross Entropy With Logits Loss: 0.3288    accuracy Score: 0.1420\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2440    accuracy Score: 0.1420\n",
      "\n",
      "\n",
      "Epoch:  16\n",
      "Binary Cross Entropy With Logits Loss: 0.3205    accuracy Score: 0.1556\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2422    accuracy Score: 0.1560\n",
      "\n",
      "\n",
      "Epoch:  18\n",
      "Binary Cross Entropy With Logits Loss: 0.3126    accuracy Score: 0.1677\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2442    accuracy Score: 0.1600\n",
      "\n",
      "\n",
      "Epoch:  20\n",
      "Binary Cross Entropy With Logits Loss: 0.3042    accuracy Score: 0.1811\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2446    accuracy Score: 0.1670\n",
      "\n",
      "\n",
      "Epoch:  22\n",
      "Binary Cross Entropy With Logits Loss: 0.2993    accuracy Score: 0.1897\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2431    accuracy Score: 0.1690\n",
      "\n",
      "\n",
      "Epoch:  24\n",
      "Binary Cross Entropy With Logits Loss: 0.2910    accuracy Score: 0.2009\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2462    accuracy Score: 0.1600\n",
      "\n",
      "\n",
      "Epoch:  26\n",
      "Binary Cross Entropy With Logits Loss: 0.2846    accuracy Score: 0.2139\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2495    accuracy Score: 0.1630\n",
      "\n",
      "\n",
      "Epoch:  28\n",
      "Binary Cross Entropy With Logits Loss: 0.2828    accuracy Score: 0.2143\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2543    accuracy Score: 0.1660\n",
      "\n",
      "\n",
      "Epoch:  30\n",
      "Binary Cross Entropy With Logits Loss: 0.2766    accuracy Score: 0.2273\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2542    accuracy Score: 0.1720\n",
      "\n",
      "\n",
      "Epoch:  32\n",
      "Binary Cross Entropy With Logits Loss: 0.2707    accuracy Score: 0.2403\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2582    accuracy Score: 0.1690\n",
      "\n",
      "\n",
      "Epoch:  34\n",
      "Binary Cross Entropy With Logits Loss: 0.2649    accuracy Score: 0.2458\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2597    accuracy Score: 0.1720\n",
      "\n",
      "\n",
      "Epoch:  36\n",
      "Binary Cross Entropy With Logits Loss: 0.2612    accuracy Score: 0.2496\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2634    accuracy Score: 0.1750\n",
      "\n",
      "\n",
      "Epoch:  38\n",
      "Binary Cross Entropy With Logits Loss: 0.2564    accuracy Score: 0.2619\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2694    accuracy Score: 0.1710\n",
      "\n",
      "\n",
      "Epoch:  40\n",
      "Binary Cross Entropy With Logits Loss: 0.2522    accuracy Score: 0.2728\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2749    accuracy Score: 0.1650\n",
      "\n",
      "\n",
      "Epoch:  42\n",
      "Binary Cross Entropy With Logits Loss: 0.2482    accuracy Score: 0.2772\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2760    accuracy Score: 0.1650\n",
      "\n",
      "\n",
      "Epoch:  44\n",
      "Binary Cross Entropy With Logits Loss: 0.2440    accuracy Score: 0.2878\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2763    accuracy Score: 0.1600\n",
      "\n",
      "\n",
      "------------------------Final Train result-------------------\n",
      "Train result\n",
      "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout = 0.65\n",
      "Binary Cross Entropy With Logits Loss: 0.2417\n",
      "Hamming Loss : 0.1134\n",
      "Jaccard Score: 0.5522\n",
      "accuracy Score: 0.2903\n",
      "Label Ranking Average Precision Score: 0.6256\n",
      "F1 Macro Score: 0.5249\n",
      "F1 Micro Score: 0.6363\n",
      "\n",
      "\n",
      "TestBinary Cross Entropy With Logits Loss: 0.2768\n",
      "Test Hamming Loss : 0.1683\n",
      "Test Jaccard Score: 0.3774\n",
      "accuracy Score: 0.1680\n",
      "Test Label Ranking Average Precision Score: 0.4752\n",
      "Test F1 Macro Score: 0.3530\n",
      "Test F1 Micro Score: 0.4463\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# bi- lstm without glove\n",
    "learning_rates = [1e-3]\n",
    "hidden_dims = [128]\n",
    "thresholds = [0.5]\n",
    "dropouts = [0.65]\n",
    "n_epoch = 45\n",
    "device = torch.device('cuda')\n",
    "all_models = []\n",
    "iter = 1\n",
    "\n",
    "#number of iterations = 3*2*2 = 12 = number of models\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for threshold in thresholds:\n",
    "            for dropout in dropouts:\n",
    "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = dropout,\n",
    "                                   bidirectional = True, use_glove = False)\n",
    "                model = model.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "                train_loss, accuracy,hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                all_models.append(model)\n",
    "                print(\"------------------------Final Train result-------------------\")\n",
    "                print(\"Train result\")\n",
    "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout = {}\".format(learning_rate, hidden_dim, threshold, dropout))\n",
    "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
    "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
    "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
    "                print(\"accuracy Score: {:.4f}\".format(max(accuracy)))\n",
    "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
    "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
    "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
    "                print(\"\\n\")\n",
    "\n",
    "                test_loss, accuracy_scores,test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
    "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
    "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
    "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
    "                print(\"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
    "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
    "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
    "                print(\"\\n\")\n",
    "                print(\"----------------------------------------------------------------\")\n",
    "                iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "senwave = pd.read_csv('senwave_preprocess_version2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "senwave = senwave[['ID','Tweet','Positive','Negative','Neutural']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sen_train, sen_test = train_test_split(senwave, train_size = 0.9, random_state = 1024)\n",
    "\n",
    "sen_train.to_csv(\"train.csv\", index = False)\n",
    "sen_test.to_csv(\"test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer(tweet):\n",
    "    tweet = re.sub(r'[\\n]', ' ', tweet)\n",
    "    return [tok.text for tok in spacy_en.tokenizer(tweet)]\n",
    "\n",
    "TWEET = torchtext.legacy.data.Field(sequential = True, lower = True, tokenize = tokenizer)\n",
    "LABEL = torchtext.legacy.data.Field(sequential = False, use_vocab = False)\n",
    "\n",
    "dataFields = [(\"ID\", None), (\"Tweet\", TWEET), (\"Positive\", LABEL), (\"Negative\", LABEL),\n",
    "              (\"Neutural\", LABEL)]\n",
    "\n",
    "train_dataset, test_dataset = torchtext.legacy.data.TabularDataset.splits(\n",
    "    path = '', train = 'train.csv', test = 'test.csv', format = 'csv', fields = dataFields, skip_header = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TWEET.build_vocab(train_dataset, vectors = 'glove.840B.300d') # get word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = TWEET.vocab # data \n",
    "BATCH_SIZE = 32\n",
    "n_label= 3\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "\n",
    "train_iter, test_iter = legacy.data.BucketIterator.splits(datasets = (train_dataset, test_dataset),\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   sort_key = lambda x : len(x.Tweet),\n",
    "                                                   sort_within_batch = False,\n",
    "                                                   repeat = False,\n",
    "                                                   device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改TWEET.vocab\n",
    "np.random.seed(1024)\n",
    "for i in range(TWEET.vocab.vectors.shape[0]):\n",
    "    vec = TWEET.vocab.vectors[i]\n",
    "    if torch.sum(vec).item() == 0:\n",
    "        a = np.random.uniform(-0.25, 0.25, 300)\n",
    "        TWEET.vocab.vectors[i] = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.legacy.data.batch.Batch of size 32]\n",
       "\t[.Tweet]:[torch.cuda.LongTensor of size 31x32 (GPU 0)]\n",
       "\t[.Positive]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Negative]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.Neutural]:[torch.cuda.LongTensor of size 32 (GPU 0)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea taken from http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "class BatchWrapper():\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl = dl\n",
    "        self.x_var = x_var\n",
    "        self.y_vars = y_vars\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var)\n",
    "            if self.y_vars is not None:\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim = 1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "            yield(x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"Tweet\", ['Positive','Negative','Neutural'])\n",
    "test_dl = BatchWrapper(test_iter, \"Tweet\", ['Positive','Negative','Neutural'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab, hidden_dim, output_dim, drop_prob, bidirectional = False, use_glove = True):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(len(vocab), embedding_dim) # vocab是数据，使用的这种embedding的方式\n",
    "        \n",
    "        if use_glove:\n",
    "            self.embeddings.weight.data.copy_(vocab.vectors) # 如果使用glove，这里其实不是很懂呢\n",
    "            self.embeddings.weight.requires_grad = False\n",
    "            \n",
    "        self.drop_prob = drop_prob # drop layer\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional = bidirectional, batch_first = True, num_layers = 2)\n",
    "        \n",
    "        if bidirectional is True:\n",
    "            self.lin = nn.Linear(2*hidden_dim, 64)\n",
    "        else:\n",
    "            self.lin = nn.Linear(hidden_dim, 64)\n",
    "        \n",
    "        self.fc = nn.Linear(64, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = drop_prob)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        #sentence = [max_len, batch_size]\n",
    "\n",
    "        embed = self.embeddings(torch.transpose(sentence, 0, 1))\n",
    "        #embed = [batch_size, max_len, embedding_dim]\n",
    "        \n",
    "        if self.drop_prob:\n",
    "            embed = self.dropout(embed)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(embed)\n",
    "        #lstm_out = [batch_size, max_len, 2*hidden_dim if bidirectional else hidden_dim]\n",
    "        #hidden = [num_layers, batch_size, hidden_dim]\n",
    "        #cell = [num_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        out = lstm_out[:,-1,:].squeeze()\n",
    "        #out = [batch_size, 2*hidden_dim if bidirectional else hidden_dim]\n",
    "        \n",
    "        out = self.lin(out)\n",
    "        #out = [batch_size, 64]\n",
    "\n",
    "        outputs = self.fc(out)\n",
    "        #outputs = [batch_size, output_dim]\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "def evaluation_metrics(actual_labels, pred_labels, threshold):\n",
    "    \n",
    "#     int_pred_labels = pred_labels\n",
    "#     for i in range(len(pred_labels)):\n",
    "#          for j in range(3):\n",
    "#             if int_pred_labels[i][j] >= threshold: \n",
    "#                 int_pred_labels[i][j] = 1\n",
    "#             else:\n",
    "#                 int_pred_labels[i][j] = 0\n",
    "    \n",
    "    int_pred_labels = pred_labels\n",
    "    for i in range(len(pred_labels)):\n",
    "        max_index = np.argmax(pred_labels[i])\n",
    "        for j in range(3):\n",
    "            if j == max_index:\n",
    "                int_pred_labels[i][j] = 1\n",
    "            else:\n",
    "                int_pred_labels[i][j] = 0\n",
    "        \n",
    "    ham_loss = hamming_loss(actual_labels, int_pred_labels)\n",
    "    accuracy_scores = accuracy_score(actual_labels, int_pred_labels)\n",
    "    jacc_score = jaccard_score(actual_labels, int_pred_labels, average = 'samples')\n",
    "    lrap = label_ranking_average_precision_score(actual_labels, pred_labels)\n",
    "    f1_macro = f1_score(actual_labels, int_pred_labels, average = 'macro')\n",
    "    f1_micro = f1_score(actual_labels, int_pred_labels, average = 'micro')\n",
    "\n",
    "    return ham_loss,accuracy_scores, jacc_score, lrap, f1_macro, f1_micro\n",
    "\n",
    "def train(model, loss_fn, optimizer, n_epochs, train_dl, threshold,test_dl):\n",
    "\n",
    "    train_losses = []  \n",
    "    accuracy = []\n",
    "    hamming_losses = []\n",
    "    jaccard_scores = []\n",
    "    lraps = []  \n",
    "    iter = 1\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        running_loss = 0.0\n",
    "        pred_labels = []\n",
    "        actual_labels = []\n",
    "        model.train()\n",
    "        for x, y in train_dl:\n",
    "            #print(x.shape, y.shape)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(x)\n",
    "\n",
    "            m =  nn.Softmax(dim=1) # 在这里取softmax\n",
    "            \n",
    "            sig_preds = m(preds) \n",
    "            for tens in sig_preds:\n",
    "                pred_labels.append(tens.cpu().detach().numpy())\n",
    "            for tens in y:\n",
    "                actual_labels.append(tens.cpu().detach().numpy())\n",
    "            #return preds,y\n",
    "            \n",
    "            y_loss = y.argmax(dim = 1)\n",
    "            loss = loss_fn(preds, y_loss)\n",
    "            \n",
    "\n",
    "            # loss = loss_fn(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x.shape[0]\n",
    "        # print(\"pred_labels\",pred_labels)\n",
    "        # print('actual_labels',actual_labels)\n",
    "        ham_loss, accuracy_scores,jacc_score, lrap, f1_macro, f1_micro = evaluation_metrics(actual_labels, pred_labels, threshold)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        accuracy.append(accuracy_scores)\n",
    "        hamming_losses.append(ham_loss)\n",
    "        lraps.append(lrap)\n",
    "        jaccard_scores.append(jacc_score)\n",
    "        \n",
    "        if iter % 2 == 0:\n",
    "            print(\"Epoch: \", epoch)\n",
    "            print(\"Binary Cross Entropy With Logits Loss: {:.4f}   \".format(epoch_loss),\n",
    "                  \"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "            test_loss, accuracy_scores,test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
    "            print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}   \".format(test_loss),\n",
    "                 \"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "            print(\"\\n\")\n",
    "        iter += 1\n",
    "\n",
    "    return train_losses,accuracy, hamming_losses, jaccard_scores, lraps, f1_macro, f1_micro\n",
    "\n",
    "def test(model, loss_fn, test_dl, threshold):\n",
    "    running_loss = 0.0\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    model.eval()\n",
    "    for x, y in test_dl:\n",
    "        #print(x.shape, y.shape)\n",
    "\n",
    "        preds = model(x) \n",
    "\n",
    "        m = nn.Softmax(dim=1) \n",
    "        sig_preds = m(preds)\n",
    "        \n",
    "        for tens in sig_preds:\n",
    "            pred_labels.append(tens.cpu().detach().numpy())\n",
    "        for tens in y:\n",
    "            actual_labels.append(tens.cpu().detach().numpy())\n",
    "\n",
    "        y_loss = y.argmax(dim = 1)\n",
    "        loss = loss_fn(preds, y_loss)\n",
    "\n",
    "        running_loss += loss.item() * x.shape[0]\n",
    "\n",
    "    ham_loss, accuracy_scores, jacc_score, lrap, f1_macro, f1_micro = evaluation_metrics(actual_labels, pred_labels, threshold)\n",
    "\n",
    "    test_loss = running_loss / len(test_dataset)\n",
    "    return test_loss,accuracy_scores, ham_loss, jacc_score, lrap, f1_macro, f1_micro\n",
    "\n",
    "\n",
    "def confusion(model, loss_fn, test_dl, threshold):\n",
    "    running_loss = 0.0\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    model.eval()\n",
    "    for x, y in test_dl:\n",
    "        #print(x.shape, y.shape)\n",
    "        preds = model(x)\n",
    "        m = nn.Softmax(dim=1) \n",
    "        sig_preds = m(preds)\n",
    "        for tens in sig_preds:\n",
    "            pred_labels.append(tens.cpu().detach().numpy())\n",
    "        for tens in y:\n",
    "            actual_labels.append(tens.cpu().detach().numpy())\n",
    "        loss = loss_fn(preds, y)\n",
    "        running_loss += loss.item() * x.shape[0]\n",
    "    \n",
    "    pred_final = []\n",
    "    for i in range(len(pred_labels)):\n",
    "        pred_final.append(np.argmax(pred_labels[i]))\n",
    "        \n",
    "    actual_final = []\n",
    "    for i in range(len(actual_labels)):\n",
    "        actual_final.append(np.argmax(actual_labels[i]))\n",
    "    \n",
    "    result = confusion_matrix(actual_final, pred_final)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lstm with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Binary Cross Entropy With Logits Loss: 0.8714    accuracy Score: 0.5789\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5753    accuracy Score: 0.5885\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Binary Cross Entropy With Logits Loss: 0.7849    accuracy Score: 0.6195\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5179    accuracy Score: 0.6254\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "Binary Cross Entropy With Logits Loss: 0.7546    accuracy Score: 0.6340\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5004    accuracy Score: 0.6298\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "Binary Cross Entropy With Logits Loss: 0.7301    accuracy Score: 0.6470\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5112    accuracy Score: 0.6232\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "Binary Cross Entropy With Logits Loss: 0.7125    accuracy Score: 0.6556\n",
      "TestBinary Cross Entropy With Logits Loss: 0.4868    accuracy Score: 0.6287\n",
      "\n",
      "\n",
      "------------------------Final Train result-------------------\n",
      "Train result\n",
      "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout = 0.65\n",
      "Binary Cross Entropy With Logits Loss: 0.7125\n",
      "Hamming Loss : 0.2296\n",
      "Jaccard Score: 0.6556\n",
      "accuracy Score: 0.6556\n",
      "Label Ranking Average Precision Score: 0.7704\n",
      "F1 Macro Score: 0.5473\n",
      "F1 Micro Score: 0.6556\n",
      "\n",
      "\n",
      "TestBinary Cross Entropy With Logits Loss: 0.4868\n",
      "Test Hamming Loss : 0.2476\n",
      "Test Jaccard Score: 0.6287\n",
      "accuracy Score: 0.6287\n",
      "Test Label Ranking Average Precision Score: 0.7524\n",
      "Test F1 Macro Score: 0.5053\n",
      "Test F1 Micro Score: 0.6287\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-3]\n",
    "hidden_dims = [128]\n",
    "thresholds = [0.5]\n",
    "dropouts = [0.65]\n",
    "n_epoch = 10\n",
    "device = torch.device('cuda')\n",
    "all_models = []\n",
    "iter = 1\n",
    "\n",
    "#number of iterations = 3*2*2 = 12 = number of models\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for threshold in thresholds:\n",
    "            for dropout in dropouts:\n",
    "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 3, drop_prob = dropout,\n",
    "                                   bidirectional = False, use_glove = True)\n",
    "                model = model.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                \n",
    "                # preds,y = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                \n",
    "                train_loss, accuracy,hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                all_models.append(model)\n",
    "                print(\"------------------------Final Train result-------------------\")\n",
    "                print(\"Train result\")\n",
    "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout = {}\".format(learning_rate, hidden_dim, threshold, dropout))\n",
    "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
    "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
    "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
    "                print(\"accuracy Score: {:.4f}\".format(max(accuracy)))\n",
    "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
    "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
    "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
    "                print(\"\\n\")\n",
    "\n",
    "                test_loss, accuracy_scores,test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
    "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
    "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
    "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
    "                print(\"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
    "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
    "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
    "                print(\"\\n\")\n",
    "                print(\"----------------------------------------------------------------\")\n",
    "                iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with out GLove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Binary Cross Entropy With Logits Loss: 0.8884    accuracy Score: 0.5630\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5849    accuracy Score: 0.5852\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Binary Cross Entropy With Logits Loss: 0.8420    accuracy Score: 0.5823\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5782    accuracy Score: 0.5928\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "Binary Cross Entropy With Logits Loss: 0.8001    accuracy Score: 0.6073\n",
      "TestBinary Cross Entropy With Logits Loss: 0.6458    accuracy Score: 0.6048\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "Binary Cross Entropy With Logits Loss: 0.7527    accuracy Score: 0.6288\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5801    accuracy Score: 0.6145\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "Binary Cross Entropy With Logits Loss: 0.7062    accuracy Score: 0.6506\n",
      "TestBinary Cross Entropy With Logits Loss: 0.6261    accuracy Score: 0.6102\n",
      "\n",
      "\n",
      "------------------------Final Train result-------------------\n",
      "Train result\n",
      "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout = 0.65\n",
      "Binary Cross Entropy With Logits Loss: 0.7062\n",
      "Hamming Loss : 0.2329\n",
      "Jaccard Score: 0.6506\n",
      "accuracy Score: 0.6506\n",
      "Label Ranking Average Precision Score: 0.7671\n",
      "F1 Macro Score: 0.5322\n",
      "F1 Micro Score: 0.6506\n",
      "\n",
      "\n",
      "TestBinary Cross Entropy With Logits Loss: 0.6261\n",
      "Test Hamming Loss : 0.2599\n",
      "Test Jaccard Score: 0.6102\n",
      "accuracy Score: 0.6102\n",
      "Test Label Ranking Average Precision Score: 0.7401\n",
      "Test F1 Macro Score: 0.4685\n",
      "Test F1 Micro Score: 0.6102\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-3]\n",
    "hidden_dims = [128]\n",
    "thresholds = [0.5]\n",
    "dropouts = [0.65]\n",
    "n_epoch = 10\n",
    "device = torch.device('cuda')\n",
    "all_models = []\n",
    "iter = 1\n",
    "\n",
    "#number of iterations = 3*2*2 = 12 = number of models\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for threshold in thresholds:\n",
    "            for dropout in dropouts:\n",
    "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 3, drop_prob = dropout,\n",
    "                                   bidirectional = False, use_glove = False)\n",
    "                model = model.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                \n",
    "                # preds,y = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                \n",
    "                train_loss, accuracy,hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                all_models.append(model)\n",
    "                print(\"------------------------Final Train result-------------------\")\n",
    "                print(\"Train result\")\n",
    "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout = {}\".format(learning_rate, hidden_dim, threshold, dropout))\n",
    "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
    "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
    "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
    "                print(\"accuracy Score: {:.4f}\".format(max(accuracy)))\n",
    "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
    "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
    "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
    "                print(\"\\n\")\n",
    "\n",
    "                test_loss, accuracy_scores,test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
    "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
    "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
    "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
    "                print(\"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
    "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
    "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
    "                print(\"\\n\")\n",
    "                print(\"----------------------------------------------------------------\")\n",
    "                iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi LSTM with GLove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Binary Cross Entropy With Logits Loss: 0.8780    accuracy Score: 0.5636\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5665    accuracy Score: 0.5668\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Binary Cross Entropy With Logits Loss: 0.8116    accuracy Score: 0.5898\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5191    accuracy Score: 0.6156\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "Binary Cross Entropy With Logits Loss: 0.7543    accuracy Score: 0.6367\n",
      "TestBinary Cross Entropy With Logits Loss: 0.4928    accuracy Score: 0.6428\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "Binary Cross Entropy With Logits Loss: 0.7293    accuracy Score: 0.6493\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5188    accuracy Score: 0.6265\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "Binary Cross Entropy With Logits Loss: 0.6999    accuracy Score: 0.6591\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5021    accuracy Score: 0.6352\n",
      "\n",
      "\n",
      "------------------------Final Train result-------------------\n",
      "Train result\n",
      "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout = 0.65\n",
      "Binary Cross Entropy With Logits Loss: 0.6999\n",
      "Hamming Loss : 0.2273\n",
      "Jaccard Score: 0.6591\n",
      "accuracy Score: 0.6591\n",
      "Label Ranking Average Precision Score: 0.7727\n",
      "F1 Macro Score: 0.5571\n",
      "F1 Micro Score: 0.6591\n",
      "\n",
      "\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5021\n",
      "Test Hamming Loss : 0.2432\n",
      "Test Jaccard Score: 0.6352\n",
      "accuracy Score: 0.6352\n",
      "Test Label Ranking Average Precision Score: 0.7568\n",
      "Test F1 Macro Score: 0.5353\n",
      "Test F1 Micro Score: 0.6352\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-3]\n",
    "hidden_dims = [128]\n",
    "thresholds = [0.5]\n",
    "dropouts = [0.65]\n",
    "n_epoch = 10\n",
    "device = torch.device('cuda')\n",
    "all_models = []\n",
    "iter = 1\n",
    "\n",
    "#number of iterations = 3*2*2 = 12 = number of models\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for threshold in thresholds:\n",
    "            for dropout in dropouts:\n",
    "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 3, drop_prob = dropout,\n",
    "                                   bidirectional = True, use_glove = True)\n",
    "                model = model.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                \n",
    "                # preds,y = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                \n",
    "                train_loss, accuracy,hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                all_models.append(model)\n",
    "                print(\"------------------------Final Train result-------------------\")\n",
    "                print(\"Train result\")\n",
    "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout = {}\".format(learning_rate, hidden_dim, threshold, dropout))\n",
    "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
    "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
    "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
    "                print(\"accuracy Score: {:.4f}\".format(max(accuracy)))\n",
    "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
    "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
    "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
    "                print(\"\\n\")\n",
    "\n",
    "                test_loss, accuracy_scores,test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
    "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
    "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
    "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
    "                print(\"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
    "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
    "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
    "                print(\"\\n\")\n",
    "                print(\"----------------------------------------------------------------\")\n",
    "                iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi lstm without glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Binary Cross Entropy With Logits Loss: 0.8874    accuracy Score: 0.5628\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5908    accuracy Score: 0.5646\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Binary Cross Entropy With Logits Loss: 0.8460    accuracy Score: 0.5711\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5931    accuracy Score: 0.5494\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "Binary Cross Entropy With Logits Loss: 0.7949    accuracy Score: 0.5996\n",
      "TestBinary Cross Entropy With Logits Loss: 0.6042    accuracy Score: 0.5733\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "Binary Cross Entropy With Logits Loss: 0.7585    accuracy Score: 0.6235\n",
      "TestBinary Cross Entropy With Logits Loss: 0.5996    accuracy Score: 0.5896\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "Binary Cross Entropy With Logits Loss: 0.7062    accuracy Score: 0.6540\n",
      "TestBinary Cross Entropy With Logits Loss: 0.6061    accuracy Score: 0.5668\n",
      "\n",
      "\n",
      "------------------------Final Train result-------------------\n",
      "Train result\n",
      "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout = 0.65\n",
      "Binary Cross Entropy With Logits Loss: 0.7062\n",
      "Hamming Loss : 0.2307\n",
      "Jaccard Score: 0.6540\n",
      "accuracy Score: 0.6540\n",
      "Label Ranking Average Precision Score: 0.7693\n",
      "F1 Macro Score: 0.5389\n",
      "F1 Micro Score: 0.6540\n",
      "\n",
      "\n",
      "TestBinary Cross Entropy With Logits Loss: 0.6061\n",
      "Test Hamming Loss : 0.2888\n",
      "Test Jaccard Score: 0.5668\n",
      "accuracy Score: 0.5668\n",
      "Test Label Ranking Average Precision Score: 0.7112\n",
      "Test F1 Macro Score: 0.4484\n",
      "Test F1 Micro Score: 0.5668\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-3]\n",
    "hidden_dims = [128]\n",
    "thresholds = [0.5]\n",
    "dropouts = [0.65]\n",
    "n_epoch = 10\n",
    "device = torch.device('cuda')\n",
    "all_models = []\n",
    "iter = 1\n",
    "\n",
    "#number of iterations = 3*2*2 = 12 = number of models\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for threshold in thresholds:\n",
    "            for dropout in dropouts:\n",
    "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 3, drop_prob = dropout,\n",
    "                                   bidirectional = True, use_glove = False)\n",
    "                model = model.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                \n",
    "                # preds,y = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                \n",
    "                train_loss, accuracy,hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer,n_epoch, train_dl, threshold,test_dl)\n",
    "                all_models.append(model)\n",
    "                print(\"------------------------Final Train result-------------------\")\n",
    "                print(\"Train result\")\n",
    "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout = {}\".format(learning_rate, hidden_dim, threshold, dropout))\n",
    "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
    "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
    "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
    "                print(\"accuracy Score: {:.4f}\".format(max(accuracy)))\n",
    "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
    "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
    "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
    "                print(\"\\n\")\n",
    "\n",
    "                test_loss, accuracy_scores,test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
    "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
    "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
    "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
    "                print(\"accuracy Score: {:.4f}\".format(accuracy_scores))\n",
    "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
    "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
    "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
    "                print(\"\\n\")\n",
    "                print(\"----------------------------------------------------------------\")\n",
    "                iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a glass of wine keeps the corona away  drake  ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can anyone tell me if you took the flu shot la...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>by the way producers send me beats im working ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when someone you know   apart of your family d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dear soccer  i really miss you  please come ba...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  \\\n",
       "0  a glass of wine keeps the corona away  drake  ...   \n",
       "1  can anyone tell me if you took the flu shot la...   \n",
       "2  by the way producers send me beats im working ...   \n",
       "3  when someone you know   apart of your family d...   \n",
       "4  dear soccer  i really miss you  please come ba...   \n",
       "\n",
       "                                list  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "1  [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "3  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = senwave.drop(['ID'], axis = 1)\n",
    "df['list'] = df[df.columns[1:12]].values.tolist()\n",
    "new_df = df[['Tweet', 'list']].copy()\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bb3520403c4d0eb494af1905f5c2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b23a87143734133a865815dc06ee339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b037dcf85a33425181a7e27fbc2f86e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 200 #based on length of tweets\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 1e-05 #tried 1e-03, 1e-04, 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataframe = dataframe\n",
    "        self.tweet = dataframe['Tweet']\n",
    "        self.targets = self.dataframe.list\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tweet = str(self.tweet[index])\n",
    "        tweet = \" \".join(tweet.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "        return {\n",
    "            'ids' : torch.tensor(ids, dtype = torch.long),\n",
    "            'mask' : torch.tensor(mask, dtype = torch.long),\n",
    "            'token_type_ids' : torch.tensor(token_type_ids, dtype = torch.long),\n",
    "            'targets' : torch.tensor(self.targets[index], dtype = torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sen_train.drop(['ID'], axis = 1)\n",
    "train_dataset['list'] = train_dataset[train_dataset.columns[1:12]].values.tolist()\n",
    "train_df = train_dataset[['Tweet', 'list']].copy()\n",
    "train_df = train_df.reset_index(drop = True)\n",
    "\n",
    "test_dataset = sen_test.drop(['ID'], axis = 1)\n",
    "test_dataset['list'] = test_dataset[test_dataset.columns[1:12]].values.tolist()\n",
    "test_df = test_dataset[['Tweet', 'list']].copy()\n",
    "test_df = test_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c22c04ce0249bfb17d1ec935892d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (layer1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Dropout(p=0.3, inplace=False)\n",
       "  (layer3): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.layer2 = torch.nn.Dropout(0.3)\n",
    "        self.layer3 = torch.nn.Linear(768, 11)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids, return_dict = False):\n",
    "        unw, out_1 = self.layer1(ids, attention_mask = mask, token_type_ids = token_type_ids)[0], self.layer1(ids, attention_mask = mask, token_type_ids = token_type_ids)[1]\n",
    "        out_2 = self.layer2(out_1)\n",
    "        out_final = self.layer3(out_2)\n",
    "        return out_final\n",
    "\n",
    "model = BERT()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for unw, data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids, return_dict = False)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if unw % 2000 == 0:\n",
    "            print(f'Iter : {unw+1}, Epoch: {epoch+1}, Loss: {total_loss/(unw+1)}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 1, Epoch: 1, Loss: 0.6962275505065918\n",
      "Iter : 2001, Epoch: 1, Loss: 0.4257115384508883\n",
      "Iter : 4001, Epoch: 1, Loss: 0.4031909396054893\n",
      "Iter : 6001, Epoch: 1, Loss: 0.38547705694563766\n",
      "Iter : 8001, Epoch: 1, Loss: 0.37646117098345455\n",
      "Iter : 1, Epoch: 2, Loss: 0.28627488017082214\n",
      "Iter : 2001, Epoch: 2, Loss: 0.30361873152731417\n",
      "Iter : 4001, Epoch: 2, Loss: 0.3006949994673031\n",
      "Iter : 6001, Epoch: 2, Loss: 0.30041549270738266\n",
      "Iter : 8001, Epoch: 2, Loss: 0.30066742445892236\n",
      "Iter : 1, Epoch: 3, Loss: 0.23145930469036102\n",
      "Iter : 2001, Epoch: 3, Loss: 0.2394077192074147\n",
      "Iter : 4001, Epoch: 3, Loss: 0.23735791795938962\n",
      "Iter : 6001, Epoch: 3, Loss: 0.23757051523148498\n",
      "Iter : 8001, Epoch: 3, Loss: 0.24030982081623184\n",
      "Iter : 1, Epoch: 4, Loss: 0.2412097454071045\n",
      "Iter : 2001, Epoch: 4, Loss: 0.17873933605723147\n",
      "Iter : 4001, Epoch: 4, Loss: 0.1792770810318206\n",
      "Iter : 6001, Epoch: 4, Loss: 0.18177160203448808\n",
      "Iter : 8001, Epoch: 4, Loss: 0.1835075834197529\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid():\n",
    "    model.eval()\n",
    "    req_targets = []\n",
    "    req_outputs = []\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for unw, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            req_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            req_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    valid_loss /= len(testing_loader)\n",
    "    return req_outputs, req_targets, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "outputs, targets, valid_loss = valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.array(outputs)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_outputs = np.zeros((outputs.shape[0], outputs.shape[1]))\n",
    "\n",
    "for row in range(outputs.shape[0]):\n",
    "    for col in range(outputs.shape[1]):\n",
    "        if outputs[row][col] >= 0.5: int_outputs[row][col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ham_loss = hamming_loss(targets, int_outputs)\n",
    "bert_jacc_score = jaccard_score(targets, int_outputs, average = 'samples')\n",
    "bert_lrap = label_ranking_average_precision_score(targets, outputs)\n",
    "bert_f1_macro = f1_score(targets, int_outputs, average = 'macro')\n",
    "bert_f1_micro = f1_score(targets, int_outputs, average = 'micro')\n",
    "accuracy_scores = accuracy_score(targets, int_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3775623864624649\n",
      "Hamming Loss: 0.14281818181818182\n",
      "Jaccard Score: 0.49894999999999995\n",
      "Label Ranking Average Precision Score: 0.7510845538720546\n",
      "F1 Macro Score: 0.5010336549884022\n",
      "F1 Micro Score: 0.5694710879693067\n",
      "acc Score: 0.232\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss:\", valid_loss)\n",
    "print(\"Hamming Loss:\", bert_ham_loss)\n",
    "print(\"Jaccard Score:\", bert_jacc_score)\n",
    "print(\"Label Ranking Average Precision Score:\", bert_lrap)\n",
    "print(\"F1 Macro Score:\", bert_f1_macro)\n",
    "print(\"F1 Micro Score:\", bert_f1_micro)\n",
    "print(\"acc Score:\", accuracy_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}